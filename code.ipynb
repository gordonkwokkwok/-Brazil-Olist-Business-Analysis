{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv files separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = pd.read_csv('.\\datasets\\olist_customers_dataset.csv')\n",
    "geo = pd.read_csv('.\\datasets\\olist_geolocation_dataset.csv')\n",
    "item = pd.read_csv('.\\datasets\\olist_order_items_dataset.csv')\n",
    "payment = pd.read_csv('.\\datasets\\olist_order_payments_dataset.csv')\n",
    "order = pd.read_csv('.\\datasets\\olist_orders_dataset.csv')\n",
    "review = pd.read_csv('.\\datasets\\olist_order_reviews_dataset.csv')\n",
    "product = pd.read_csv('.\\datasets\\olist_products_dataset.csv')\n",
    "seller = pd.read_csv('.\\datasets\\olist_sellers_dataset.csv')\n",
    "prodinfo = pd.read_csv('.\\datasets\\product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. olist_customers_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the text\n",
    "customer['customer_city'] = customer['customer_city'].str.title()\n",
    "customer['customer_state'] = customer['customer_state'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. olist_geolocation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = geo.drop_duplicates()\n",
    "geo = geo.groupby('geolocation_zip_code_prefix')[['geolocation_lat', 'geolocation_lng']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. olist_order_items_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column datatype from string to datetime\n",
    "item['shipping_limit_date'] = pd.to_datetime(item['shipping_limit_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. olist_order_payment_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. olist_order_reviews_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change datatype from string to datetime\n",
    "review['review_creation_date'] = pd.to_datetime(review['review_creation_date'])\n",
    "review['review_answer_timestamp'] = pd.to_datetime(review['review_answer_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the equipment insufficient, we cannot translate 100,000 reviews. Thus, we only translate the message which length>200.\n",
    "# select all message which length>200\n",
    "\n",
    "top_n_longest_review = []\n",
    "top_n_longest_review_score = []\n",
    "\n",
    "def appendFuction(x):\n",
    "    \n",
    "    '''return the message which is score>x and its length>200'''\n",
    "\n",
    "    for i in range(len(review['review_score'])):\n",
    "        if review2['review_score'][i]==x and len(review['review_comment_message'][i])>200:\n",
    "            top_n_longest_review.append(review2['review_comment_message'][i])\n",
    "            top_n_longest_review_score.append(x)\n",
    "\n",
    "appendFuction(5.0)\n",
    "appendFuction(4.0)\n",
    "appendFuction(3.0)\n",
    "appendFuction(2.0)\n",
    "appendFuction(1.0)\n",
    "\n",
    "data = {'review_comment_message':top_n_longest_review, 'review_score': top_n_longest_review_score}\n",
    "longest_eng_message = pd.DataFrame(data)\n",
    "\n",
    "# Translation\n",
    "longest_eng_message['review_comment_message'] = longest_eng_message['review_comment_message'].apply(lambda x: GoogleTranslator(source='pt', target='en').translate(x))\n",
    "longest_eng_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the most comment keyword in all review message\n",
    "n1 = 300\n",
    "p = Counter(\" \".join(review[\"review_comment_message\"]).split()).most_common(n1)\n",
    "\n",
    "# Create a DataFrame for the top n1-keyword\n",
    "most_common_n_word = pd.DataFrame(p, columns=['word', 'frequency'])\n",
    "\n",
    "# Since the translation cannot translate object for integer, we have to convert the 'word' column to str\n",
    "most_common_n_word[\"word\"] = most_common_n_word[\"word\"].astype(str)\n",
    "\n",
    "# We now drop the word in which length is less than n2\n",
    "n2 = 7\n",
    "for i in range(len(most_common_n_word[\"word\"])):\n",
    "    if len(str(most_common_n_word[\"word\"][i]))<7:\n",
    "        most_common_n_word = most_common_n_word.drop(i)\n",
    "\n",
    "# Since we drop the certain rows, we have to handle the index matter\n",
    "most_common_n_word = most_common_n_word.reset_index()\n",
    "most_common_n_word = most_common_n_word.drop('index',axis=1)\n",
    "\n",
    "# Lowercase the string\n",
    "most_common_n_word['word'] = most_common_n_word['word'].str.lower()\n",
    "\n",
    "# Start translation\n",
    "most_common_n_word['word'] = most_common_n_word['word'].apply(lambda x: GoogleTranslator(source='pt', target='en').translate(x))\n",
    "\n",
    "# Since the are some duplicates after translation, we first drop the certain rows, we have to handle the index matter\n",
    "most_common_n_word = most_common_n_word.drop_duplicates(subset=['word'])\n",
    "most_common_n_word = most_common_n_word.reset_index()\n",
    "most_common_n_word = most_common_n_word.drop('index',axis=1)\n",
    "\n",
    "\n",
    "# Let see the DataFrame \n",
    "pd.set_option('display.max_rows', None)\n",
    "most_common_n_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. olist_orders_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change datatype from string to datetime\n",
    "order['order_purchase_timestamp'] = pd.to_datetime(order['order_purchase_timestamp'])\n",
    "order['order_approved_at'] = pd.to_datetime(order['order_approved_at'])\n",
    "order['order_delivered_carrier_date'] = pd.to_datetime(order['order_delivered_carrier_date'])\n",
    "order['order_delivered_customer_date'] = pd.to_datetime(order['order_delivered_customer_date'])\n",
    "order['order_estimated_delivery_date'] = pd.to_datetime(order['order_estimated_delivery_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. olist_products_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype 3 columns from float to integer\n",
    "product = product.fillna(0)\n",
    "for column in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty']:\n",
    "    product[column] = product[column].astype('int')\n",
    "\n",
    "# Normalize the text, and thus easy to read in PowerBI \n",
    "product['product_category_name'] = product['product_category_name'].str.replace('_', ' ').str.lower()\n",
    "\n",
    "# Change the datatype of \"product_category_name\" columns from regular string to 74 categorical variables\n",
    "product['product_category_name'] = product['product_category_name'].astype('category')\n",
    "\n",
    "# Translation\n",
    "product['product_category_name'] = product['product_category_name'].apply(lambda x: GoogleTranslator(source='pt', target='en').translate(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. olist_sellers_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace value from '04482255' to 'unknown'\n",
    "seller['seller_city'] = seller['seller_city'].replace('04482255','unknown')\n",
    "\n",
    "# Translation\n",
    "seller['seller_city'] = seller['seller_city'].apply(lambda x: GoogleTranslator(source='pt', target='en').translate(x))\n",
    "\n",
    "# Normalize the text \n",
    "seller['seller_city'] = seller['seller_city'].str.title()\n",
    "seller['seller_state'] = seller['seller_state'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. product_category_name_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text \n",
    "prodinfo['product_category_name'] = prodinfo['product_category_name'].str.replace('_', ' ').str.lower()\n",
    "prodinfo['product_category_name_english'] = prodinfo['product_category_name_english'].str.replace('_', ' ').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import cleaned dataframe into mySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect engine to mysql database\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"your user name\",\n",
    "                               pw=\"your password\",\n",
    "                               db=\"your database name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe into mysql database\n",
    "customer.to_sql('customer', con=engine)\n",
    "geo.to_sql('geo', con=engine)\n",
    "item.to_sql('item', con=engine)\n",
    "payment.to_sql('payment', con=engine)\n",
    "order.to_sql('order', con=engine)\n",
    "review.to_sql('review', con=engine)\n",
    "product.to_sql('product', con=engine)\n",
    "seller.to_sql('seller', con=engine)\n",
    "prodinfo.to_sql('prodinfo', con=engine)\n",
    "\n",
    "longest_eng_message.to_sql('longest_eng_message', con=engine)\n",
    "most_common_n_word.to_sql('most_common_n_word', con=engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
